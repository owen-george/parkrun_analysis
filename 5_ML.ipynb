{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2203b715-8f24-4fd8-9f6e-75dd5e591a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error, make_scorer\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d49a154-a9bf-4354-9932-693be362b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean/parkrun_weather_v2.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3223b371-1174-4465-a610-c9e6974acfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Position', 'Position_score', 'Name', 'Runner_id',\n",
       "       'Parkrun_count', 'Gender', 'Age_group', 'Time_in_minutes',\n",
       "       'Total_Appearances', 'Appearance_Instance', 'Appearance/Total',\n",
       "       'Appearance_Category', 'Total_event_runners', 'Days_since_last_parkrun',\n",
       "       'PB_mins', 'ave_mins', 'prev_PB', 'avg_prev_run_time', 'temperature',\n",
       "       'windspeed', 'precipitation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07e64f0-7e09-45b5-b29d-92c914aeba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the 'Age_group' to a numeric value (you could use the starting age of each range)\n",
    "age_group_map = {\n",
    "    '18-19': 19,\n",
    "    '20-24': 22,\n",
    "    '25-29': 27,\n",
    "    '30-34': 32,\n",
    "    '35-39': 37,\n",
    "    '40-44': 42,\n",
    "    '45-49': 47,\n",
    "    '50-54': 52,\n",
    "    '55-59': 57,\n",
    "    '60-64': 62,\n",
    "    '65-69': 67,\n",
    "    '70-74': 72\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Age_group' column\n",
    "df['Age_group_numeric'] = df['Age_group'].map(age_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a0f241a-aea9-4051-876c-7cbd74a3ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_parkrun_date'] = df.groupby('Runner_id')['Date'].transform('min')\n",
    "df['Days_since_first_parkrun'] = (df['Date'] - df['first_parkrun_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d8132b8-0a95-41c4-832d-b37998c4bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Male'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eef6bc7d-5ace-4564-bdf5-328bc27cc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'Runner_id' and 'Date'\n",
    "df = df.sort_values(by=['Runner_id', 'Date'])\n",
    "df['Previous_time_mins'] = df.groupby('Runner_id')['Time_in_minutes'].shift(1)\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37654334-d159-4857-854e-0e20b63ec9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 157970 entries, 9 to 165553\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Non-Null Count   Dtype         \n",
      "---  ------                    --------------   -----         \n",
      " 0   Date                      157970 non-null  datetime64[ns]\n",
      " 1   Position                  157970 non-null  int64         \n",
      " 2   Position_score            157970 non-null  float64       \n",
      " 3   Name                      157970 non-null  object        \n",
      " 4   Runner_id                 157970 non-null  int64         \n",
      " 5   Parkrun_count             157970 non-null  int64         \n",
      " 6   Gender                    157970 non-null  object        \n",
      " 7   Age_group                 157970 non-null  object        \n",
      " 8   Time_in_minutes           157970 non-null  float64       \n",
      " 9   Total_Appearances         157970 non-null  int64         \n",
      " 10  Appearance_Instance       157970 non-null  int64         \n",
      " 11  Appearance/Total          157970 non-null  float64       \n",
      " 12  Appearance_Category       157970 non-null  object        \n",
      " 13  Total_event_runners       157970 non-null  int64         \n",
      " 14  Days_since_last_parkrun   157970 non-null  float64       \n",
      " 15  PB_mins                   157970 non-null  float64       \n",
      " 16  ave_mins                  157970 non-null  float64       \n",
      " 17  prev_PB                   157970 non-null  float64       \n",
      " 18  avg_prev_run_time         157970 non-null  float64       \n",
      " 19  temperature               157970 non-null  float64       \n",
      " 20  windspeed                 157970 non-null  float64       \n",
      " 21  precipitation             157970 non-null  float64       \n",
      " 22  Age_group_numeric         157970 non-null  int64         \n",
      " 23  first_parkrun_date        157970 non-null  datetime64[ns]\n",
      " 24  Days_since_first_parkrun  157970 non-null  int64         \n",
      " 25  Male                      157970 non-null  int64         \n",
      " 26  Previous_time_mins        157970 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(12), int64(9), object(4)\n",
      "memory usage: 33.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28cd570c-a777-47b0-8a25-1af6dfbe783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target and non-numeric columns\n",
    "X = df.drop(columns=['Time_in_minutes', 'Parkrun_count', 'Gender', 'PB_mins', 'ave_mins', 'Position_score',\n",
    "                     'Age_group', 'Date', 'Name', 'Position', 'Total_Appearances', 'Appearance/Total',\n",
    "                     'first_parkrun_date', 'Runner_id','Appearance_Category'])\n",
    "y = df['Time_in_minutes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9257caf-02b1-4db4-9d9d-e48a58bc1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Step 2: Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Step 3: Normalize the training data\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Optionally, you can also scale the test data using the same scaler\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# Convert the normalized data back to DataFrame\n",
    "X_train_norm_df = pd.DataFrame(X_train_norm, columns=X.columns)\n",
    "X_test_norm_df = pd.DataFrame(X_test_norm, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65abafa1-c27c-4db4-80a1-e46be397e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Function to plot feature importance of a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model (e.g., RandomForestRegressor)\n",
    "    feature_names: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    feature_importances\n",
    "    \"\"\"\n",
    "    feature_importances = model.feature_importances_\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_names, feature_importances)  # Horizontal bar plot for better readability\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    return feature_importances\n",
    "\n",
    "def plot_predicted_vs_actual(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot scatter plot of predicted vs actual values.\n",
    "    \n",
    "    Parameters:\n",
    "    y_test: Actual target values\n",
    "    y_pred: Predicted target values\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Line for perfect prediction\n",
    "    plt.xlabel('Actual Run Time (minutes)')\n",
    "    plt.ylabel('Predicted Run Time (minutes)')\n",
    "    plt.title('Scatter Plot: Predicted vs Actual Run Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9b00483-8e21-478c-a1b2-a3b70e31a8b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize and train Random Forest model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(X_train_norm_df, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test_norm_df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[0;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[0;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize and train Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(y_test, y_pred_rf)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse.round(3)}')\n",
    "print(f'R-squared: {r2.round(3)}')\n",
    "\n",
    "# Call functions to visualize feature importance and predicted vs actual\n",
    "rf_features = plot_feature_importance(rf, X_train_norm_df.columns)\n",
    "plot_predicted_vs_actual(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760addd-d4a7-4a79-bef4-f5156a64cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Linear Regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = lr.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_lr = root_mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f'Linear Regression - RMSE: {rmse_lr.round(3)}')\n",
    "print(f'Linear Regression - R-squared: {r2_lr.round(3)}')\n",
    "\n",
    "# Visualize results\n",
    "plot_predicted_vs_actual(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ee176-0063-49c3-ae4e-741366504c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Gradient Boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=0)\n",
    "gb.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_gb = gb.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_gb = root_mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f'Gradient Boosting - RMSE: {rmse_gb.round(3)}')\n",
    "print(f'Gradient Boosting - R-squared: {r2_gb.round(3)}')\n",
    "\n",
    "# Visualize results\n",
    "gb_features = plot_feature_importance(gb, X_train_norm_df.columns)\n",
    "plot_predicted_vs_actual(y_test, y_pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b24a0e-a058-4869-962c-2855fe14b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train XGBoost model\n",
    "xg = xgb.XGBRegressor(n_estimators=100, random_state=0)\n",
    "xg.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xg = xg.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_xg = root_mean_squared_error(y_test, y_pred_xg)\n",
    "r2_xg = r2_score(y_test, y_pred_xg)\n",
    "\n",
    "print(f'XGBoost - RMSE: {rmse_xg.round(3)}')\n",
    "print(f'XGBoost - R-squared: {r2_xg.round(3)}')\n",
    "\n",
    "# Visualize results\n",
    "xg_features = plot_feature_importance(xg, X_train_norm_df.columns)\n",
    "plot_predicted_vs_actual(y_test, y_pred_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac967070-ba69-4e6e-9cc1-1d1159bf498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train SVR model\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_svr = svr.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_svr = root_mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f'Support Vector Regression - RMSE: {rmse_svr.round(3)}')\n",
    "print(f'Support Vector Regression - R-squared: {r2_svr.round(3)}')\n",
    "\n",
    "# Visualize results\n",
    "plot_predicted_vs_actual(y_test, y_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fa4be-ad1f-4c36-a48b-37c73003d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train KNN model\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn = knn.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_knn = root_mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f'KNN - RMSE: {rmse_knn.round(3)}')\n",
    "print(f'KNN - R-squared: {r2_knn.round(3)}')\n",
    "\n",
    "# Visualize results\n",
    "plot_predicted_vs_actual(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852783be-800d-4243-a645-a83375b1d995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'KNN - RMSE: {rmse_knn.round(3)}')\n",
    "print(f'KNN - R-squared: {r2_knn.round(3)}')\n",
    "print(f'Support Vector Regression - RMSE: {rmse_svr.round(3)}')\n",
    "print(f'Support Vector Regression - R-squared: {r2_svr.round(3)}')\n",
    "print(f'XGBoost - RMSE: {rmse_xg.round(3)}')\n",
    "print(f'XGBoost - R-squared: {r2_xg.round(3)}')\n",
    "print(f'Gradient Boosting - RMSE: {rmse_gb.round(3)}')\n",
    "print(f'Gradient Boosting - R-squared: {r2_gb.round(3)}')\n",
    "print(f'Linear Regression - RMSE: {rmse_lr.round(3)}')\n",
    "print(f'Linear Regression - R-squared: {r2_lr.round(3)}')\n",
    "print(f'Random Forest - RMSE: {rmse.round(3)}')\n",
    "print(f'Random Forest - R-squared: {r2.round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2bf09-1f02-41ed-95d5-ebac340ebbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=params, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train_norm_df, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3e303-010c-47a9-8e52-e1d9fdd38683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = XGBRegressor(**best_params, random_state=0)\n",
    "best_model.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xg = best_model.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_xg = root_mean_squared_error(y_test, y_pred_xg)\n",
    "r2_xg = r2_score(y_test, y_pred_xg)\n",
    "\n",
    "print(f\"Test RMSE: {rmse_xg:.3f}\")\n",
    "print(f\"Test R-squared: {r2_xg:.3f}\")\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76feda-7bb8-4fd7-afdf-985ac5428887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "    }\n",
    "\n",
    "    # Create and evaluate the model\n",
    "    model = XGBRegressor(**params, random_state=0)\n",
    "    scores = cross_val_score(model, X_train_norm_df, y_train, \n",
    "                             cv=5, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "    \n",
    "    return np.mean(scores)  # Return negative MSE for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0211025-3845-49ae-8d70-7dfba868c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # Adjust n_trials as needed\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best RMSE:\", np.sqrt(-study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db4eb-2cc3-41de-9bf7-efd6d82182d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_model = XGBRegressor(**best_params, random_state=0)\n",
    "best_model.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test_norm_df)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test RMSE: {rmse:.3f}\")\n",
    "print(f\"Test R-squared: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de2a98-4042-4cef-9eac-03a68a3e0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study).show()\n",
    "plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47768b89-7464-4060-86c9-605e73e9c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the optimal parameters\n",
    "best_params = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.012243304825496931,\n",
    "    'subsample': 0.865760407475865,\n",
    "    'colsample_bytree': 0.8515210990372176,\n",
    "    'gamma': 1.7316033622064344,\n",
    "    'reg_alpha': 2.1905117158413243,\n",
    "    'reg_lambda': 6.278342959785368\n",
    "}\n",
    "\n",
    "# Train the model with the best parameters\n",
    "opt_xgb = XGBRegressor(**best_params, random_state=0)\n",
    "opt_xgb.fit(X_train_norm_df, y_train)\n",
    "\n",
    "# Predict and evaluate on the train set\n",
    "y_pred_tr_opt_x = opt_xgb.predict(X_train_norm_df)\n",
    "rmse_opt_x = root_mean_squared_error(y_train, y_pred_tr_opt_x )\n",
    "r2_opt_x = r2_score(y_train, y_pred_tr_opt_x )\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_opt_x = opt_xgb.predict(X_test_norm_df)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_opt_x = root_mean_squared_error(y_test, y_pred_opt_x)\n",
    "r2_opt_x = r2_score(y_test, y_pred_opt_x)\n",
    "\n",
    "\n",
    "print(f\"Train RMSE: \")\n",
    "print(f\"Train R-Squared: \")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Test RMSE: {rmse_opt_x:.3f}\")\n",
    "print(f\"Test R-squared: {r2_opt_x:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_predicted_vs_actual(y_test, y_pred_opt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4cb09a-da9a-432d-a302-3a26549016e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
