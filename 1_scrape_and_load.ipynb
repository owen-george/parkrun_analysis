{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203b715-8f24-4fd8-9f6e-75dd5e591a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c9ce9-3408-455f-826a-02dfa1ab6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_parkrun_data(x, y, location='brighton', existing_df=None):\n",
    "    \"\"\"\n",
    "    Fetches parkrun data from the given page range and appends it to an existing dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: int, the starting page number (inclusive).\n",
    "    - y: int, the ending page number (inclusive).\n",
    "    - location: The parkrun location as it appears in: https://www.parkrun.org.uk/{location}/results/\n",
    "            - If nothing is provided it defaults to \"brighton\"\n",
    "    - existing_df: pandas DataFrame (optional), existing dataframe to append data to. If None, a new DataFrame is created.\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame containing the fetched data with duplicates removed.\n",
    "    \"\"\"\n",
    "    y += 1\n",
    "    \n",
    "    # Column names for the DataFrame\n",
    "    columns = ['Date', 'Position', 'Name', 'Runner_id', 'Parkrun_count', 'Gender', 'Age_group', 'Time']\n",
    "    data = []\n",
    "    \n",
    "    # Loop over the range of pages from x to y\n",
    "    for i in range(x, y):\n",
    "\n",
    "        url = f'https://www.parkrun.org.uk/{location}/results/{i}/'\n",
    "        print(f\"Processing page {i - x + 1}/{y - x}...            \", end=\"\\r\")  # Updates on status\n",
    "        # Set up headers to avoid blocking by the website\n",
    "        headers = {\n",
    "             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edge/110.0.1587.56',  # Updated User-Agent for newer browsers\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'DNT': '1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'TE': 'Trailers',\n",
    "            'Pragma': 'no-cache',\n",
    "            'Referer': 'https://www.parkrun.org.uk/',\n",
    "            'Origin': 'https://www.parkrun.org.uk',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'If-None-Match': 'W/\"f0b3eb46c6c7e1f04161c38a1f041f4\"'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Request the page content\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Check for any HTTP errors\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Extract the parkrun date from the page\n",
    "            prun_date = soup.select('span.format-date')[0].get_text(strip=True)\n",
    "\n",
    "            # Loop through each row of results\n",
    "            for row in soup.select('tr.Results-table-row'):\n",
    "                if row.get('data-name') != 'Unknown':  # Filter out \"Unknown\" names\n",
    "                    position = row.get('data-position')\n",
    "                    prun_count = row.get('data-runs')\n",
    "                    name = row.get('data-name')\n",
    "                    gender = row.get('data-gender')\n",
    "                    age_group = row.get('data-agegroup')\n",
    "                    time = row.find('td', class_='Results-table-td--time').find('div', class_='compact').get_text(strip=True).split(\"\\n\")[0]\n",
    "                    runner_id_tag = row.find('a', href=True)\n",
    "                    runner_id = runner_id_tag['href'].split('/')[3] if runner_id_tag else None\n",
    "                    \n",
    "                    # Append the extracted data as a list\n",
    "                    data.append([prun_date, position, name, runner_id, prun_count, gender, age_group, time])\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # If there's an error loading the page, print the error and continue with the next page\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert the collected data into a DataFrame\n",
    "    new_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate it with the new data and remove duplicates\n",
    "    if existing_df is not None:\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        return combined_df\n",
    "    else:\n",
    "        # If no existing DataFrame is provided, return the new DataFrame\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb3a28-7a42-4d15-b925-f0d95c302274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the page will give a 405 error. In that case you may need to go to the site and complete a captcha. \n",
    "# Save the dataframe up to the stage it completed to .csv, then close and reload the notebook\n",
    "# Run the notebook again from where the error began, inputting the done notebook to concat to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ef953-10df-42b8-97d2-0fd0700900f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_parkrun_data(1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38e121-ad4b-4825-a443-3f76095f6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_parkrun_data(201, 771, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9636de-b81a-453f-ae93-ac196f2d0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_parkrun_data(772, 826, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa5748-9e5c-45ee-b99a-077f88dd1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/raw/parkrun_full_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
